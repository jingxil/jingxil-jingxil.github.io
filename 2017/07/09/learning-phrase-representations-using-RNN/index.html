
 <!DOCTYPE HTML>
<html lang="default">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>Learning Phrase Representations Using RNN | CN Yah</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="Jingxi Liang">
    
    <meta name="description" content="This post is a review of the work [1] of  Kyunghyun Cho et al.">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/pacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/pacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="CN Yah" title="CN Yah"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="CN Yah">CN Yah</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					
						<li><a href="/">Home</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="text" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:cnyah.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/07/09/learning-phrase-representations-using-RNN/" title="Learning Phrase Representations Using RNN" itemprop="url">Learning Phrase Representations Using RNN</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://cnyah.com" title="Jingxi Liang">Jingxi Liang</a>
    </p>
  <p class="article-time">
    <time datetime="2017-07-09T23:06:04.000Z" itemprop="datePublished">2017-09-07</time>
    Updated:<time datetime="2017-07-17T07:09:23.411Z" itemprop="dateModified">2017-17-07</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Overview"><span class="toc-number">1.</span> <span class="toc-text">Overview</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Model"><span class="toc-number">2.</span> <span class="toc-text">The Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Enhance-Phrase-based-Translation-Model"><span class="toc-number">3.</span> <span class="toc-text">Enhance Phrase-based Translation Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Phrase-Representations"><span class="toc-number">4.</span> <span class="toc-text">Phrase Representations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Notes"><span class="toc-number">5.</span> <span class="toc-text">Notes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">6.</span> <span class="toc-text">References</span></a></li></ol>
		</div>
		
		<p>This post is a review of the work [1] of  Kyunghyun Cho et al. </p>
<a id="more"></a>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>In Kyunghyun Cho et al.’s work [1], they proposed a RNN Encoder-Decoder model which can be adapted to</p>
<ol>
<li>Cast a linguistic phrase to a dense continuous vector representation.</li>
<li>Score a given pair of input and output sequences to enhance the performance of a statistical machine translation (MT) system. </li>
<li>Generate a target sequence given a input sequence. </li>
</ol>
<p>In this work, the focus is mainly on the second type of usage. But I am more interested in the first kind of usage. I am going to briefly go through how RNN Encoder-Decoder model improves the performance of MT system and demonstrate the semantically and syntactically meaningful phrase vector representations  RNN Encoder-Decoder model can learn.</p>
<h2 id="The-Model"><a href="#The-Model" class="headerlink" title="The Model"></a>The Model</h2><p>I assume readers are familiar with the Seq2Seq model which was proposed by Ilya Sutskever et al. [2] based on the model here. The major differences are</p>
<ol>
<li>Seq2Seq model reverses the order of input sequence while RNN Encoder-Decoder model is not.</li>
<li>Seq2Seq model uses LSTM as its RNN unit while the model here uses GRU. (introduction about LSTM and GRU can be found <a href="http://cnyah.com/2017/06/26/gated-recurrent-neural-networks/">here</a>)</li>
</ol>
<h2 id="Enhance-Phrase-based-Translation-Model"><a href="#Enhance-Phrase-based-Translation-Model" class="headerlink" title="Enhance Phrase-based Translation Model"></a>Enhance Phrase-based Translation Model</h2><p>RNN  Encoder-Decoder is trained on bilingual corpora where the  frequencies of each phrase pair are ignored. Due to the reasons: 1) reduce the computetional expense 2) make sure the RNN Encoder-Decoder does not simply learn to rank the phrase pairs according to their numbers of occurances. 3) ensure most of capacity of the model is focused toward learning linguistic regularities.</p>
<p>Trained RNN Encoder–Decoder adds a new score for each phrase pair to the existing phrase table where the score is simply the probability p( <strong>o</strong> | <strong>i</strong> ) here. The new score as a feature is used by existing tuning algorithms.</p>
<p>Compared with baseline system Moses [3], the translation model adding the feature from RNN  performances better. Interestingly, According to Figure 1, despite many phrase pairs were scored similarly by both translation model and the RNN Encoder–Decoder, there were as many pairs that were scored radically different. They accounted it for that the RNN ignores the frequencies of phrase pairs from the corpus.</p>
<p><img src="figure-1.png" alt="figure-1"></p>
<p>Figure 1 is from [1].</p>
<h2 id="Phrase-Representations"><a href="#Phrase-Representations" class="headerlink" title="Phrase Representations"></a>Phrase Representations</h2><p>Since the model is first to encode the input into length-fixed vector, this vector is potientially capable to represent the input. In K. Cho et al.’s experienment [1], this vector is 1000-dimensional. Figures below demonstrate both semantic and syntactic structures of the phrases is captured by RNN Encoder-Decoder. </p>
<p><img src="word-representations-1.png" alt="word-representations-1"></p>
<p>Figure 2: it is from [1] and shows the relative distances between learned word representations.</p>
<p><img src="phrase-representations-1.png" alt="phrase-representations-1"></p>
<p>Figure 3: it is from [1] and shows the relative distances between learned phrase representations.</p>
<p><img src="phrase-representations-2.png" alt="phrase-representations-2"></p>
<p>Figure 4: it is from [1] and shows the relative distances between learned phrase representations. </p>
<h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><ul>
<li>Instead of recovering input as Word2Vec approaches do, RNN Encoder-Decoder here builds word vectors by taking English phrases as input and producing corresponding French phrases.</li>
<li>They mentioned that GRU can be regarded as a variant of a leaky-integration unit.</li>
<li>They were not able to get meaningful result with an oft-used tanh unit without any gating.</li>
<li>I personally feel like that using bilingual corpus can accelerate learning semantics of words. </li>
<li>I wonder whether it is possible to segment Chinese words using similar approach on English-Chinese corpora.</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Kyunghyun Cho et al. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation, 2014<br>[2] Ilya Sutskever et al. Sequence to Sequence Learning with Neural Networks, 2014<br>[3] Moses, <a href="http://www.statmt.org/moses/" target="_blank" rel="external">http://www.statmt.org/moses/</a></p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/summary/">summary</a><a href="/tags/word-embeddings/">word embeddings</a>
  </div>




<div class="article-share" id="share">

  <div data-url="http://cnyah.com/2017/07/09/learning-phrase-representations-using-RNN/" data-title="Learning Phrase Representations Using RNN | CN Yah" data-tsina="null" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/07/16/pointer-networks/" title="Pointer Networks">
  <strong>PREVIOUS:</strong><br/>
  <span>
  Pointer Networks</span>
</a>
</div>


<div class="next">
<a href="/2017/07/09/the-evolution-of-vector-word-representations/"  title="The Evolution of Vector Word Representations">
 <strong>NEXT:</strong><br/> 
 <span>The Evolution of Vector Word Representations
</span>
</a>
</div>

</nav>

	
</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Overview"><span class="toc-number">1.</span> <span class="toc-text">Overview</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Model"><span class="toc-number">2.</span> <span class="toc-text">The Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Enhance-Phrase-based-Translation-Model"><span class="toc-number">3.</span> <span class="toc-text">Enhance Phrase-based Translation Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Phrase-Representations"><span class="toc-number">4.</span> <span class="toc-text">Phrase Representations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Notes"><span class="toc-number">5.</span> <span class="toc-text">Notes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">6.</span> <span class="toc-text">References</span></a></li></ol>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  

  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			<li><a href="/tags/RNN/" title="RNN">RNN<sup>1</sup></a></li>
		
			<li><a href="/tags/Seq2Seq/" title="Seq2Seq">Seq2Seq<sup>1</sup></a></li>
		
			<li><a href="/tags/notes/" title="notes">notes<sup>3</sup></a></li>
		
			<li><a href="/tags/seq2seq/" title="seq2seq">seq2seq<sup>1</sup></a></li>
		
			<li><a href="/tags/summary/" title="summary">summary<sup>3</sup></a></li>
		
			<li><a href="/tags/weekly/" title="weekly">weekly<sup>1</sup></a></li>
		
			<li><a href="/tags/word-embeddings/" title="word embeddings">word embeddings<sup>2</sup></a></li>
		
		</ul>
</div>


  
  <div class="archiveslist">
    <p class="asidetitle"><a href="/archives">Archives</a></p>
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a><span class="archive-list-count">2</span></li></ul>
  </div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<div class="social-font clearfix">
		
		
		
		
		
	</div>
		<p class="copyright">Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/A-limon/pacman" target="_blank" title="Pacman">Pacman</a> © 2017 
		
		<a href="http://cnyah.com" target="_blank" title="Jingxi Liang">Jingxi Liang</a>
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->






  </body>
</html>

