
 <!DOCTYPE HTML>
<html lang="default">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>Methods to Calculate Sentence Smilarity | CN Yah</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="Jingxi Liang">
    
    <meta name="description" content="Calculating the similarity between sentences is not an easy task. To tackle this task, we first need to give each sentence a representation (eg. a vec">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/pacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/pacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="CN Yah" title="CN Yah"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="CN Yah">CN Yah</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					
						<li><a href="/">Home</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="text" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:cnyah.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/05/07/methods-to-calculate-sentence-smilarity/" title="Methods to Calculate Sentence Smilarity" itemprop="url">Methods to Calculate Sentence Smilarity</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://cnyah.com" title="Jingxi Liang">Jingxi Liang</a>
    </p>
  <p class="article-time">
    <time datetime="2018-05-07T00:06:04.000Z" itemprop="datePublished">2018-07-05</time>
    Updated:<time datetime="2018-05-07T09:05:51.000Z" itemprop="dateModified">2018-07-05</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Unsupervised-Methods"><span class="toc-number">1.</span> <span class="toc-text">Unsupervised Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Ngram"><span class="toc-number">1.1.</span> <span class="toc-text">Ngram</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TF-IDF"><span class="toc-number">1.2.</span> <span class="toc-text">TF-IDF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WordVec"><span class="toc-number">1.3.</span> <span class="toc-text">WordVec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SentVec"><span class="toc-number">1.4.</span> <span class="toc-text">SentVec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AutoEncoder"><span class="toc-number">1.5.</span> <span class="toc-text">AutoEncoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sequential-Denoising-AutoEncoder"><span class="toc-number">1.5.1.</span> <span class="toc-text">Sequential Denoising AutoEncoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Recurrent-Variational-AutoEncoder"><span class="toc-number">1.5.2.</span> <span class="toc-text">Recurrent Variational AutoEncoder</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Other-Methods"><span class="toc-number">2.</span> <span class="toc-text">Other Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Unupervised-Methods-Depending-on-Additional-Information"><span class="toc-number">2.1.</span> <span class="toc-text">Unupervised Methods Depending on Additional Information</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Supervised-Methods"><span class="toc-number">2.2.</span> <span class="toc-text">Supervised Methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">3.</span> <span class="toc-text">References</span></a></li></ol>
		</div>
		
		<p>Calculating the similarity between sentences is not an easy task. To tackle this task, we first need to give each sentence a representation (eg. a vector) and then use a measurement (eg. cosine distance) to compute the distance between every sentence pair. Based on whether the dataset is labeled, those methods could be categorized into unsupervised methods and supervised methods. Since the labeled data is very scarce, researchers have more interested in unsupervised methods.</p>
<h2 id="Unsupervised-Methods"><a href="#Unsupervised-Methods" class="headerlink" title="Unsupervised Methods"></a>Unsupervised Methods</h2><h3 id="Ngram"><a href="#Ngram" class="headerlink" title="Ngram"></a>Ngram</h3><p>The ngram is a very trivial method. Extract n-gram from the sentence and count how many n-grams they have in common. For 1-gram, we can also apply Longest Common Subsequence (LCS) to take the 1-gram order into consideration.</p>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>the TF-IDF calculates the score of each word which is a combination of the Term Frequency(TF) and Inverse Document Frequency(IDF). We first transform the sentence to the bag of words. Then a sentence vector $\vec{s}$ whose length is as large as the vocabulary size is constructed. We set each dimension of $\vec{s}$ to the corresponding TF-IDF value or 0 if the word does not appear in the sentence. Now, we can use cosine distance to determine the similarity between sentences. </p>
<h3 id="WordVec"><a href="#WordVec" class="headerlink" title="WordVec"></a>WordVec</h3><p>The WordVec is a learned vectorized representation of words. There exist various methods to train word vectors and we can use trained word vectors to represent sentence vectors. </p>
<p>The first one is to simply average word vectors plainly[1] or with weight[2] in a sentence. The second one is to use Word Mover’s Distance(WMD)[1] to measure the similarity which is derived from the well-studied Earth Mover’s Distance. The WMD calculates the total cost of sentence A being transformed to sentence B. The larger the cost from A to B, the less similar between A and B. </p>
<p><img src="wmd.png" alt="wmd.png"><br>Figure 1 Word Mover’s Distance (from [1])</p>
<p>The WMD is based on word travel cost $c(i,j)$ which is measured by the Euclidean distance between the two word vectors $||\vec{w}_i-\vec{w}_j||_2$. To handle  the repeated occurrence of words in one sentence, the sentence is represented as normalized bag-of-words. For each word $i$, $d_i$ denotes the word frequency in the sentence. To measure the WMD of words in A being transformed into B, a ﬂow matrix $T$ where $T_{ij} &gt;= 0 $ denotes how much of word i in A travels to word j in B is needed. This matrix $T$ satisfies 1) $\sum_{j} T_{ij} = d^A_{i}$, the outgoing ﬂow from word i equals $d^A_{i}$ , 2) $\sum_{i} T_{ij} = d^B_{j}$, the incoming ﬂow to word j equals $d^B_{j}$. Hence, the target here is to find the $T$ that<br>$$<br>\mbox{min} \sum ^{n}_{i,j=1} T_{ij} c(i,j)<br>$$</p>
<p>After getting the $T^*$, the WMD is $\sum ^{n}_{i,j=1} T^{*}_{ij} c(i,j)$. More details such a fast distance computation could be seen in [3].</p>
<p><img src="wmd-example.png" alt="wmd-example.png"><br>Figure 2 Two Word Mover’s Distance Examples (from [3])</p>
<h3 id="SentVec"><a href="#SentVec" class="headerlink" title="SentVec"></a>SentVec</h3><p>The specific SentVec model introduced here is proposed by Matteo Pagliardini et al. [4] which could be interpreted as “a natural extension of the word contexts from C-BOW to a larger sentence context”. However, it augments the sentence embeddings by n-gram embeddings.</p>
<p>Like other matrix factor models, it has the optimization form<br>$$<br>\mathop{min}_{U,V} \sum_{S \in C} f_{S}(UV\iota_{S})<br>$$<br>where $U \in R^{k \times h}$ and $V \in R^{k \times |\nu|} $  are parameter matrices. For a given sentence $S$, the indicator vector $\iota_{S} \in \{0, 1\}^{|\nu|}$ is a binary vector encoding. This equation above is used to learn source $\vec{v}_w$ and target $\vec{u}_w$ embeddings from V and U respectively. The sentence embedding $\vec{v}_S$ is obtained as the average of the source word embeddings of its constituent words which not only contains 1-gram but also n-gram embeddings. It is modeled as<br>$$<br>\vec{v}_S := \frac{1}{|R(S)|} \sum_{w \in R(S)} \vec{v}_w<br>$$<br>where $R(S)$ is the list of n-grams.</p>
<h3 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h3><p>The AutoEncoder learns a hidden representation by first compressing data and then recovering it. For sequential data, the AutoEncoder is usually based on the Seq2Seq architecture.</p>
<h4 id="Sequential-Denoising-AutoEncoder"><a href="#Sequential-Denoising-AutoEncoder" class="headerlink" title="Sequential Denoising AutoEncoder"></a>Sequential Denoising AutoEncoder</h4><p>The DAE model would corrupt the high-dimensional input data according to some noise function and then try to reconstruct it. Felix Hill et al. adopts a noise function $N(S|p_{o}, p_{x} )$ in which $p_{o}, p_{x} \in [0,1]$ are free parameters the former of which is the probability to drop out a word $w$ in a sentence and the latter of which is the probability to swap $w_{i}$ and $w_{i+1}$. Interestingly, compared the model without fixed pre-trained word embeddings, the model with it is not always better. Also, it is observed that unsupervised word-order-free models (BOW) have a better performance than unsupervised word-order-sensitive models (RNN). </p>
<h4 id="Recurrent-Variational-AutoEncoder"><a href="#Recurrent-Variational-AutoEncoder" class="headerlink" title="Recurrent Variational AutoEncoder"></a>Recurrent Variational AutoEncoder</h4><p>The promising part of the RVAE is that it theoritically would put similiar sentence vectors close in continuous space. The loss term would prevent models from cheating by just “memorizing” input data. The way to enforce this constraint is to assume that a RVAE contains a probability model of data $x$ and latent variable $z$ $p(x,z)=p(x|z)p(z)$. Each data $x_i$ is generated by 1) draw a sample $ z_i \sim p(z) $  2) draw a sample from $ p(x|z)$ and it is $ x_i$.</p>
<p>To do obtain a good latent variable $z$ given $x$ , we need to compute<br>$$<br>p(z|x) = \frac{p(x|z)p(z)}{p(x)}<br>$$<br>However, calculating $p(x)$ is intractable since it needs to marginalize out the latent variables. Instead, Variational inference approximates the posterior with a family of distributions $q_{\lambda}(z|x)$. In order to ensure the $q_{\lambda}(z|x)$ approximates the true posterior $p(z|x)$, we use Kullback-Leibler divergence to measure the difference between the two distribution. Hence, we want to find a $q_{\lambda}(z|x)$ that<br>$$<br>min_{\lambda} KL(q_{\lambda}(z|x)  ||  p(z|x) )<br>$$<br>Once again, $p(z|x)$ occurs in the equation which we do not want. We rewrite the KL divergence as<br>$$<br> KL(q_{\lambda}(z|x)  ||  p(z|x) ) -  E_q[\log q_{\lambda}(z|x) ] +  E_q[\log p(x,z) ] = \log p(x)<br>$$<br>Notice that for data x, $\log p(x) $ is a constant and KL divergence is always greater than or equal to 0. Hence, minimizing the KL divergence is equivalent to maximizing the rest part. We call the rest part evidence lower bound(ELBO).<br>$$<br>ELBO(\lambda) = E_q[\log p(x,z) ] -  E_q[\log q_{\lambda}(z|x) ]<br>$$<br>Rewrite $ELBO(\lambda)$, we have<br>$$<br>ELBO(\lambda) = E_q[\log p(x|z) ] - KL(q_{\lambda}(z|x) || p(z) )<br>$$</p>
<p>In the RVAE settings, $p$ is the family of Gaussian distributions. The latent variable $z$ is sampled from a Gaussian distribution with mean 0 and std 1. The encoder learns the Gaussian distribution $p(x|z)$ and the decoder learns the distribution $q_{\lambda}(z|x)$. For each data $x$, we can use the mean $\mu$ of probability distribution $q_{\lambda}(z|x)$ as the vector representation.</p>
<p><img src="rvae.png" alt="rvae.png"><br>Figure 3 Structure of recurrent variational autoencoder (from [6])</p>
<h2 id="Other-Methods"><a href="#Other-Methods" class="headerlink" title="Other Methods"></a>Other Methods</h2><h3 id="Unupervised-Methods-Depending-on-Additional-Information"><a href="#Unupervised-Methods-Depending-on-Additional-Information" class="headerlink" title="Unupervised Methods Depending on Additional Information"></a>Unupervised Methods Depending on Additional Information</h3><p>SkipThought, ParaVec, FastSent and Siamese C-BOW are unupervised methods depending on sentence ordering.</p>
<h3 id="Supervised-Methods"><a href="#Supervised-Methods" class="headerlink" title="Supervised Methods"></a>Supervised Methods</h3><p>All models (see <a href="https://nlp.stanford.edu/projects/snli/" target="_blank" rel="external">model list</a>) aimed to solve Natural Language Inference (NLI) problem can be applied to sentence similarity task.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Wieting et al. Towards universal paraphrastic sentence embeddings<br>[2] Arora et al. A simple but tough-to-beat baseline for sentence embeddings<br>[3] Matt J. Kusner et al. From Word Embeddings To Document Distances<br>[4] Matteo Pagliardini et al. Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features<br>[5] Felix Hill et al. Learning Distributed Representations of Sentences from Unlabelled Data<br>[6] Samuel R. Bowman et al. Generating Sentences from a Continuous Space</p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/summary/">summary</a>
  </div>




<div class="article-share" id="share">

  <div data-url="http://cnyah.com/2018/05/07/methods-to-calculate-sentence-smilarity/" data-title="Methods to Calculate Sentence Smilarity | CN Yah" data-tsina="null" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 

<div class="next">
<a href="/2018/02/03/S-NET/"  title="S-NET: Extract and Generate">
 <strong>NEXT:</strong><br/> 
 <span>S-NET: Extract and Generate
</span>
</a>
</div>

</nav>

	
</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Unsupervised-Methods"><span class="toc-number">1.</span> <span class="toc-text">Unsupervised Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Ngram"><span class="toc-number">1.1.</span> <span class="toc-text">Ngram</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TF-IDF"><span class="toc-number">1.2.</span> <span class="toc-text">TF-IDF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WordVec"><span class="toc-number">1.3.</span> <span class="toc-text">WordVec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SentVec"><span class="toc-number">1.4.</span> <span class="toc-text">SentVec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AutoEncoder"><span class="toc-number">1.5.</span> <span class="toc-text">AutoEncoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sequential-Denoising-AutoEncoder"><span class="toc-number">1.5.1.</span> <span class="toc-text">Sequential Denoising AutoEncoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Recurrent-Variational-AutoEncoder"><span class="toc-number">1.5.2.</span> <span class="toc-text">Recurrent Variational AutoEncoder</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Other-Methods"><span class="toc-number">2.</span> <span class="toc-text">Other Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Unupervised-Methods-Depending-on-Additional-Information"><span class="toc-number">2.1.</span> <span class="toc-text">Unupervised Methods Depending on Additional Information</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Supervised-Methods"><span class="toc-number">2.2.</span> <span class="toc-text">Supervised Methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">3.</span> <span class="toc-text">References</span></a></li></ol>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  

  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			<li><a href="/tags/CV/" title="CV">CV<sup>1</sup></a></li>
		
			<li><a href="/tags/GAN/" title="GAN">GAN<sup>1</sup></a></li>
		
			<li><a href="/tags/NLI/" title="NLI">NLI<sup>1</sup></a></li>
		
			<li><a href="/tags/NN/" title="NN">NN<sup>1</sup></a></li>
		
			<li><a href="/tags/POS-tagging/" title="POS tagging">POS tagging<sup>1</sup></a></li>
		
			<li><a href="/tags/RL/" title="RL">RL<sup>1</sup></a></li>
		
			<li><a href="/tags/RNN/" title="RNN">RNN<sup>4</sup></a></li>
		
			<li><a href="/tags/chatbot/" title="chatbot">chatbot<sup>1</sup></a></li>
		
			<li><a href="/tags/collections/" title="collections">collections<sup>3</sup></a></li>
		
			<li><a href="/tags/machine-comprehension/" title="machine comprehension">machine comprehension<sup>4</sup></a></li>
		
			<li><a href="/tags/notes/" title="notes">notes<sup>1</sup></a></li>
		
			<li><a href="/tags/others/" title="others">others<sup>2</sup></a></li>
		
			<li><a href="/tags/project/" title="project">project<sup>1</sup></a></li>
		
			<li><a href="/tags/seq2seq/" title="seq2seq">seq2seq<sup>9</sup></a></li>
		
			<li><a href="/tags/summary/" title="summary">summary<sup>21</sup></a></li>
		
			<li><a href="/tags/word-embeddings/" title="word embeddings">word embeddings<sup>2</sup></a></li>
		
		</ul>
</div>


  
  <div class="archiveslist">
    <p class="asidetitle"><a href="/archives">Archives</a></p>
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a><span class="archive-list-count">2</span></li></ul>
  </div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<div class="social-font clearfix">
		
		
		
		<a href="https://github.com/jingxil" target="_blank" title="github"></a>
		
		
		
	</div>
		<p class="copyright">Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/A-limon/pacman" target="_blank" title="Pacman">Pacman</a> © 2018 
		
		<a href="http://cnyah.com" target="_blank" title="Jingxi Liang">Jingxi Liang</a>
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>





<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-50558696-1', 'cnyah.com');  
ga('send', 'pageview');
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  </body>
</html>

